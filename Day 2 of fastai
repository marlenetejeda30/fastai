Today I learned a ot about NN. Though I have not been able to get the app running. 

Code I used today:

from duckduckgo_search import ddg_images
from fastcore.all import *

from fastdownload import download_url

from fastai.vision.all import *

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))


def search_images(term,max_images=30):
    print(f"Searching for '{term}'")
    return L(ddg_images(term,max_results=max_images)).itemgot('image')

search_terms_bear=['grizzly bear', 'grizzly bear cub', 'grizzly bear in the wild']

search_terms_cat=['persian cat','siamese cat', 'maine coon cat', 'bengal cat']


from time import sleep

path = Path("/kaggle/working/images/")

def process_images(path,label,o):
    dest=(path/label)
    dest.mkdir(exist_ok=True, parents=True)
    download_images(dest, urls= search_images(o))
    
results_bear= list(map(lambda x: process_images(path,"bear",x), search_terms_bear))
results_cat= list(map(lambda x: process_images(path,"cat",x), search_terms_cat))


failed_cats = verify_images(get_image_files(dest_folder_cats))
failed_cats.map(Path.unlink)

failed= verify_images(get_image_files(path))
failed.map(Path.unlink)
len(failed)


dls = DataBlock(
    blocks=(ImageBlock, CategoryBlock), 
    get_items=get_image_files, 
    splitter=RandomSplitter(valid_pct=0.2, seed=42),
    get_y=parent_label,
    item_tfms=[Resize(192, method='squish')]
).dataloaders(path, bs=32)





proccess images function takes in the path, the label (cat or bear) and the search_terms_brear(o). this will create the destination where images will be placed
then will make the directories, then download them. Map function takes the function and apply search_terms_bear or cat. Another important thing we want to check is
to see whether or not any images failed and we want to remove/unlink them for our images.

Next DataBlock: takes blocks= which is the type of file you giving it and the type of Y
get_images( path where files are)
splitter(this will split our data into 80% training and 20% validation and seed=42 for replication
get_y=parent_label (Specifies how to extract the labels from the file paths)
 item_tfms=[Resize(192, method='squish')] Applies transformations to each image. 
In this case, we are resizing the images to a target size of 192 pixels while maintaining the aspect ratio using the 'squish' method.

From what I understand, DataBlock creates the foundation and DataLoader applies it. Formally it is defined as: The .dataloaders() method is responsible 
for creating the actual data loaders that the neural network will use during training and validation.
We are appliying to the images in path and we want batches of 32.


Questions and Answers from Chapter 2:

1. Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.

-Bad image quality(taken at night, bad camera, etc)

2.Where do text models currently have a major deficiency?

-Generating correct information


3.What are possible negative societal implications of text generation models?

- Fake news

4.In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?

-Create better models,Models reinforce bias (like gender bias, racial bias) in training data


5.What kind of tabular data is deep learning particularly good at?
-natural language, or high cardinality categorical columns (containing larger number of discrete choices like zip code).

6.What's a key downside of directly using a deep learning model for recommendation systems?
-it recommends things based on same author for example

7.What are the steps of the Drivetrain Approach?
- 

8.How do the steps of the Drivetrain Approach map to a recommendation system?

9.Create an image recognition model using data you curate, and deploy it on the web.
-See above

10.What is DataLoaders?
- It is essentially a class that stores the data you need


11.What four things do we need to tell fastai to create DataLoaders?
-splitter
-get_images
-blocks
-item_transformation
-y_label


12.What does the splitter parameter to DataBlock do?
- says how to split the data

13.How do we ensure a random split always gives the same validation set?
- using a seed

14.What letters are often used to signify the independent and dependent variables?
- x is independent. y is dependent.

15.What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?
-crop: cuts off a part, squich- literally squished the image, pad-pad is an alternative Resize()

16.What is data augmentation? Why is it needed?
- Data augmentation is needed because you can make it learn by rotating and making the image in a lot of different ways. It will recognize in all sort of ways. 


17.What is the difference between item_tfms and batch_tfms?
-item_tfms are transformations applied to a single data sample x 
-Batch_tfms are applied to batched data samples


18.What is a confusion matrix?
- it is similar to a heat map. Gives TP, FP, FF, TT

19.What does export save?
- a pickle


20.What is it called when we use a model for getting predictions, instead of training?
-Inference

21.What are IPython widgets?
-IPython widgets are JavaScript and Python combined functionalities that let us build and interact with GUI components 


22.When might you want to use CPU for deployment? When might GPU be better?


23.What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?


24.What are three examples of problems that could occur when rolling out a bear warning system in practice?


25.What is "out-of-domain data"?


26.What is "domain shift"?


27.What are the three steps in the deployment process?

